{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 204: Sequence to Sequence Networks with Text Data\n",
    "\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "This hands-on tutorial will take you through both the basics of sequence-to-sequence networks, and how to implement them in the Microsoft Cognitive Toolkit. In particular, we will implement a sequence-to-sequence model with attention to perform grapheme to phoneme translation. We will start with some basic theory and then explain the data in more detail, and how you can download it.\n",
    "\n",
    "Andrej Karpathy has a [nice visualization](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) of five common paradigms of neural network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/paradigms.jpg\" width=\"750\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url=\"http://cntk.ai/jup/paradigms.jpg\", width=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to be talking about the fourth paradigm: many-to-many where the length of the output does not necessarily equal the length of the input, also known as sequence-to-sequence networks. The input is a sequence with a dynamic length, and the output is also a sequence with some dynamic length. It is the logical extension of the many-to-one paradigm in that previously we were predicting some category (which could easily be one of `V` words where `V` is an entire vocabulary) and now we want to predict a whole sequence of those categories.\n",
    "\n",
    "The applications of sequence-to-sequence networks are nearly limitless. It is a natural fit for machine translation (e.g. English input sequences, French output sequences); automatic text summarization (e.g. full document input sequence, summary output sequence); word to pronunciation models (e.g. character [grapheme] input sequence, pronunciation [phoneme] output sequence); and even parse tree generation (e.g. regular text input, flat parse tree output).\n",
    "\n",
    "## Basic theory\n",
    "\n",
    "A sequence-to-sequence model consists of two main pieces: (1) an encoder; and (2) a decoder. Both the encoder and the decoder are recurrent neural network (RNN) layers that can be implemented using a vanilla RNN, an LSTM, or GRU Blocks (here we will use LSTM). In the basic sequence-to-sequence model, the encoder processes the input sequence into a fixed representation that is fed into the decoder as a context. The decoder then uses some mechanism (discussed below) to decode the processed information into an output sequence. The decoder is a language model that is augmented with some \"strong context\" by the encoder, and so each symbol that it generates is fed back into the decoder for additional context (like a traditional LM). For an English to German translation task, the most basic setup might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/s2s.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 2\n",
    "Image(url=\"http://cntk.ai/jup/s2s.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic sequence-to-sequence network passes the information from the encoder to the decoder by initializing the decoder RNN with the final hidden state of the encoder as its initial hidden state. The input is then a \"sequence start\" tag (`<s>` in the diagram above) which primes the decoder to start generating an output sequence. Then, whatever word (or note or image, etc.) it generates at that step is fed in as the input for the next step. The decoder keeps generating outputs until it hits the special \"end sequence\" tag (`</s>` above).\n",
    "\n",
    "A more complex and powerful version of the basic sequence-to-sequence network uses an attention model. While the above setup works well, it can start to break down when the input sequences get long. At each step, the hidden state `h` is getting updated with the most recent information, and therefore `h` might be getting \"diluted\" in information as it processes each token. Further, even with a relatively short sequence, the last token will always get the last say and therefore the thought vector will be somewhat biased/weighted towards that last word. To deal with this problem, we use an \"attention\" mechanism that allows the decoder to look not only at all of the hidden states from the input, but it also learns which hidden states, for each step in decoding, to put the most weight on. In this tutorial we will implement a sequence-to-sequence network that can be run either with or without attention enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cntk.ai/jup/cntk204_s2s2.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 3\n",
    "Image(url=\"https://cntk.ai/jup/cntk204_s2s2.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Attention` layer above takes the current value of the hidden state in the Decoder, all of the hidden states in the Encoder, and calculates an augmented version of the hidden state to use. More specifically, the contribution from the Encoder's hidden states will represent a weighted sum of all of its hidden states where the highest weight corresponds both to the biggest contribution to the augmented hidden state and to the hidden state that will be most important for the Decoder to consider when generating the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Grapheme-to-Phoneme Conversion\n",
    "\n",
    "The [grapheme](https://en.wikipedia.org/wiki/Grapheme) to [phoneme](https://en.wikipedia.org/wiki/Phoneme) problem is a translation task that takes the letters of a word as the input sequence (the graphemes are the smallest units of a writing system) and outputs the corresponding phonemes; that is, the units of sound that make up a language. In other words, the system aims to generate an unambigious representation of how to pronounce a given input word.\n",
    "\n",
    "### Example\n",
    "\n",
    "The graphemes or the letters are translated into corresponding phonemes: \n",
    "\n",
    "> **Grapheme** : **|** T **|** A **|** N **|** G **|** E **|** R **|**  \n",
    "**Phonemes** : **|** ~T **|** ~AE **|** ~NG **|** ~ER **|**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Model Structure\n",
    "\n",
    "As discussed above, the task we are interested in solving is creating a model that takes some sequence as an input, and generates an output sequence based on the contents of the input. The model's job is to learn the mapping from the input sequence to the output sequence that it will generate. The job of the encoder is to come up with a good representation of the input that the decoder can use to generate a good output. For both the encoder and the decoder, the LSTM does a good job at this.\n",
    "\n",
    "Note that the LSTM is simply one of a whole set of different types of Blocks that can be used to implement an RNN. This is the code that is run for each step in the recurrence. In the Layers library, there are three built-in recurrent Blocks: the (vanilla) `RNN`, the `GRU`, and the `LSTM`. Each processes its input slightly differently and each has its own benefits and drawbacks for different types of tasks and networks. To get these blocks to run for each of the elements recurrently in a network, we create a `Recurrence` over them. This \"unrolls\" the network to the number of steps that are in the given input for the RNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing CNTK and other useful libraries\n",
    "\n",
    "CNTK is a Python module that contains several submodules like `io`, `learner`, `graph`, etc. We make extensive use of numpy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a test environment\n",
    "def isTest():\n",
    "    return ('TEST_DEVICE' in os.environ) \n",
    "\n",
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for CNTK version\n",
    "if not C.__version__ == \"2.0\":\n",
    "    raise Exception(\"this notebook was designed to work with 2.0. Current Version: \" + C.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "In this tutorial we will use a lightly pre-processed version of the CMUDict (version 0.7b) dataset from http://www.speech.cs.cmu.edu/cgi-bin/cmudict. The CMUDict data refers to the Carnegie Mellon University Pronouncing Dictionary and is an open-source machine-readable pronunciation dictionary for North American English. The data is in the CNTKTextFormatReader format. Here is an example sequence pair from the data, where the input sequence (S0) is in the left column, and the output sequence (S1) is on the right:\n",
    "\n",
    "```\n",
    "0\t|S0 3:1  |# <s>\t  |S1 3:1 |# <s>\n",
    "0\t|S0 4:1  |# A\t    |S1 32:1 |# ~AH\n",
    "0\t|S0 5:1  |# B\t    |S1 36:1 |# ~B\n",
    "0\t|S0 4:1  |# A\t    |S1 31:1 |# ~AE\n",
    "0\t|S0 7:1  |# D\t    |S1 38:1 |# ~D\n",
    "0\t|S0 12:1 |# I\t    |S1 47:1 |# ~IY\n",
    "0\t|S0 1:1  |# </s>\t |S1 1:1 |# </s>\n",
    "```\n",
    "\n",
    "The code below will download the required files (training, testing, the single sequence above for visual validation, and a small vocab file) and put them in a local folder (the training file is ~34 MB, testing is ~4MB, and the validation file and vocab file are both less than 1KB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download: cmudict-0.7b.test.ctf\n",
      "Download completed\n",
      "Starting download: cmudict-0.7b.train-dev-20-21.ctf\n",
      "Download completed\n",
      "Starting download: tiny.ctf\n",
      "Download completed\n",
      "Starting download: cmudict-0.7b.mapping\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "MODEL_DIR = \".\"\n",
    "DATA_DIR = os.path.join('..', 'Examples', 'SequenceToSequence', 'CMUDict', 'Data')\n",
    "# If above directory does not exist, just use current.\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    DATA_DIR = '.'\n",
    "\n",
    "dataPath = {\n",
    "  'validation': 'tiny.ctf',\n",
    "  'training': 'cmudict-0.7b.train-dev-20-21.ctf',\n",
    "  'testing': 'cmudict-0.7b.test.ctf',\n",
    "  'vocab_file': 'cmudict-0.7b.mapping',\n",
    "}\n",
    "\n",
    "for k in sorted(dataPath.keys()):\n",
    "    path = os.path.join(DATA_DIR, dataPath[k])\n",
    "    if os.path.exists(path):\n",
    "        print(\"Reusing locally cached:\", path)\n",
    "    else:\n",
    "        print(\"Starting download:\", dataPath[k])\n",
    "        url = \"https://github.com/Microsoft/CNTK/blob/v2.0/Examples/SequenceToSequence/CMUDict/Data/%s?raw=true\"%dataPath[k]\n",
    "        download(url, path)\n",
    "        print(\"Download completed\")\n",
    "    dataPath[k] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader\n",
    "\n",
    "To efficiently collect our data, randomize it for training, and pass it to the network, we use the CNTKTextFormat reader. We will create a small function that will be called when training (or testing) that defines the names of the streams in our data, and how they are referred to in the raw training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to load the model vocabulary file\n",
    "def get_vocab(path):\n",
    "    # get the vocab for printing output sequences in plaintext\n",
    "    vocab = [w.strip() for w in open(path).readlines()]\n",
    "    i2w = { i:w for i,w in enumerate(vocab) }\n",
    "    w2i = { w:i for i,w in enumerate(vocab) }\n",
    "    \n",
    "    return (vocab, i2w, w2i)\n",
    "\n",
    "# Read vocabulary data and generate their corresponding indices\n",
    "vocab, i2w, w2i = get_vocab(dataPath['vocab_file'])\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        features = StreamDef(field='S0', shape=input_vocab_dim, is_sparse=True),\n",
    "        labels   = StreamDef(field='S1', shape=label_vocab_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 69\n",
      "First 15 letters are:\n",
      "[\"'\", '</s>', '<s/>', '<s>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
      "\n",
      "Print dictionary with the vocabulary mapping:\n",
      "{0: \"'\", 1: '</s>', 2: '<s/>', 3: '<s>', 4: 'A', 5: 'B', 6: 'C', 7: 'D', 8: 'E', 9: 'F', 10: 'G', 11: 'H', 12: 'I', 13: 'J', 14: 'K', 15: 'L', 16: 'M', 17: 'N', 18: 'O', 19: 'P', 20: 'Q', 21: 'R', 22: 'S', 23: 'T', 24: 'U', 25: 'V', 26: 'W', 27: 'X', 28: 'Y', 29: 'Z', 30: '~AA', 31: '~AE', 32: '~AH', 33: '~AO', 34: '~AW', 35: '~AY', 36: '~B', 37: '~CH', 38: '~D', 39: '~DH', 40: '~EH', 41: '~ER', 42: '~EY', 43: '~F', 44: '~G', 45: '~HH', 46: '~IH', 47: '~IY', 48: '~JH', 49: '~K', 50: '~L', 51: '~M', 52: '~N', 53: '~NG', 54: '~OW', 55: '~OY', 56: '~P', 57: '~R', 58: '~S', 59: '~SH', 60: '~T', 61: '~TH', 62: '~UH', 63: '~UW', 64: '~V', 65: '~W', 66: '~Y', 67: '~Z', 68: '~ZH'}\n"
     ]
    }
   ],
   "source": [
    "input_vocab_dim  = 69\n",
    "label_vocab_dim  = 69\n",
    "\n",
    "# Print vocab and the correspoding mapping to the phonemes\n",
    "print(\"Vocabulary size is\", len(vocab))\n",
    "print(\"First 15 letters are:\")\n",
    "print(vocab[:15])\n",
    "print()\n",
    "print(\"Print dictionary with the vocabulary mapping:\")\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above to create a reader for our training data. Let's create it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        features = C.io.StreamDef(field='S0', shape=input_vocab_dim, is_sparse=True),\n",
    "        labels   = C.io.StreamDef(field='S1', shape=label_vocab_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "# Train data reader\n",
    "train_reader = create_reader(dataPath['training'], True)\n",
    "\n",
    "# Validation data reader\n",
    "valid_reader = create_reader(dataPath['validation'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's set our model hyperparameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a number of settings that control the complexity of our network, the shapes of our inputs, and other options such as whether we will use an embedding (and what size to use), and whether or not we will employ attention. We set them now as they will be made use of when we build the network graph in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "attention_dim = 128\n",
    "attention_span = 20\n",
    "attention_axis = -3\n",
    "use_attention = True\n",
    "use_embedding = True\n",
    "embedding_dim = 200\n",
    "vocab = ([w.strip() for w in open(dataPath['vocab_file']).readlines()]) # all lines of vocab_file in a list\n",
    "length_increase = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set two more parameters now: the symbols used to denote the start of a sequence (sometimes called 'BOS') and the end of a sequence (sometimes called 'EOS'). In this case, our sequence-start symbol is the tag $<s>$ and our sequence-end symbol is the end-tag $</s>$.\n",
    "\n",
    "Sequence start and end tags are important in sequence-to-sequence networks for two reasons. The sequence start tag is a \"primer\" for the decoder; in other words, because we are generating an output sequence and RNNs require some input, the sequence start token \"primes\" the decoder to cause it to emit its first generated token. The sequence end token is important because the decoder will learn to output this token when the sequence is finished. Otherwise the network wouldn't know how long of a sequence to generate. For the code below, we setup the sequence start symbol as a `Constant` so that it can later be passed to the Decoder LSTM as its `initial_state`. Further, we get the sequence end symbol's index so that the Decoder can use it to know when to stop generating tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_start =C.Constant(np.array([w=='<s>' for w in vocab], dtype=np.float32))\n",
    "sentence_end_index = vocab.index('</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: setup the input to the network\n",
    "\n",
    "### Dynamic axes in CNTK (Key concept)\n",
    "\n",
    "One of the important concepts in understanding CNTK is the idea of two types of axes:\n",
    "- **static axes**, which are the traditional axes of a variable's shape, and\n",
    "- **dynamic axes**, which have dimensions that are unknown until the variable is bound to real data at computation time.\n",
    "\n",
    "The dynamic axes are particularly important in the world of recurrent neural networks. Instead of having to decide a maximum sequence length ahead of time, padding your sequences to that size, and wasting computation, CNTK's dynamic axes allow for variable sequence lengths that are automatically packed in minibatches to be as efficient as possible.\n",
    "\n",
    "When setting up sequences, there are *two dynamic axes* that are important to consider. The first is the *batch axis*, which is the axis along which multiple sequences are batched. The second is the dynamic axis particular to that sequence. The latter is specific to a particular input because of variable sequence lengths in your data. For example, in sequence to sequence networks, we have two sequences: the **input sequence**, and the **output (or 'label') sequence**. One of the things that makes this type of network so powerful is that the length of the input sequence and the output sequence do not have to correspond to each other. Therefore, both the input sequence and the output sequence require their own unique dynamic axis.\n",
    "\n",
    "We first create the `inputAxis` for the input sequence and the `labelAxis` for the output sequence. We then define the inputs to the model by creating sequences over these two unique dynamic axes. Note that `InputSequence` and `LabelSequence` are *type declarations*. This means that the `InputSequence` is a type that consists of a sequence over the `inputAxis` axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source and target inputs to the model\n",
    "inputAxis = C.Axis('inputAxis')\n",
    "labelAxis = C.Axis('labelAxis')\n",
    "InputSequence = C.layers.SequenceOver[inputAxis]\n",
    "LabelSequence = C.layers.SequenceOver[labelAxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: define the network\n",
    "\n",
    "As discussed before, the sequence-to-sequence network is, at its most basic, an RNN (LSTM) encoder followed by an RNN (LSTM) decoder, and a dense output layer. We will implement both the Encoder and the Decoder using the CNTK Layers library. Both of these will be created as CNTK Functions. Our `create_model()` Python function creates both the `encode` and `decode` CNTK Functions. The `decode` function directly makes use of the `encode` function and the return value of `create_model()` is the CNTK Function `decode` itself.\n",
    "\n",
    "We start by passing the input through an embedding (learned as part of the training process). So that this function can be used in the `Sequential` block of the Encoder and the Decoder whether we want an embedding or not, we will use the `identity` function if the `use_embedding` parameter is `False`. We then declare the Encoder layers as follows:\n",
    "\n",
    "First, we pass the input through our `embed` function and then we stabilize it. This adds an additional scalar parameter to the learning that can help our network converge more quickly during training. Then, for each of the number of LSTM layers that we want in our encoder, except the final one, we set up an LSTM recurrence. The final recurrence will be a `Fold` if we are not using attention because we only pass the final hidden state to the decoder. If we are using attention, however, then we use another normal LSTM `Recurrence` that the Decoder will put its attention over later on.\n",
    "\n",
    "Below we see a diagram of how the layered version of the sequence-to-sequence network with attention works. As the code shows below, the output of each layer of the Encoder and Decoder is used as the input to the layer just above it. The Attention model focuses on the top layer of the Encoder and informs the first layer of the Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cntk.ai/jup/cntk204_s2s3.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 4\n",
    "Image(url=\"https://cntk.ai/jup/cntk204_s2s3.png\", width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder, we first define several sub-layers: the `Stabilizer` for the decoder input, the `Recurrence` blocks for each of the decoder's layers, the `Stabilizer` for the output of the stack of LSTMs, and the final `Dense` output layer. If we are using attention, then we also create an `AttentionModel` function `attention_model` which returns an augmented version of the decoder's hidden state with emphasis placed on the encoder hidden states that should be most used for the given step while generating the next output token.\n",
    "\n",
    "We then build the CNTK Function `decode`. The decorator `@Function` turns a regular Python function into a proper CNTK Function with the given arguments and return value. The Decoder works differently during training than it does during test time. During training, the history (i.e. input) to the Decoder `Recurrence` consists of the ground-truth labels. This means that while generating $y^{(t=2)}$, for example, the input will be $y^{(t=1)}$. During evaluation, or \"test time\", however, the input to the Decoder will be the actual output of the model. For a greedy decoder -- which we are implementing here -- that input is therefore the `hardmax` of the final `Dense` layer.\n",
    "\n",
    "The Decoder Function `decode` takes two arguments: (1) the `input` sequence; and (2) the Decoder `history`. First, it runs the `input` sequence through the Encoder function `encode` that we setup earlier. We then get the `history` and map it to its embedding if necessary. Then the embedded representation is stabilized before running it through the Decoder's `Recurrence`. For each layer of `Recurrence`, we run the embedded `history` (now represented as `r`) through the `Recurrence`'s LSTM. If we are not using attention, we run it through the `Recurrence` with its initial state set to the value of the final hidden state of the encoder (note that since we run the Encoder backwards when not using attention that the \"final\" hidden state is actually the first hidden state in chronological time). If we are using attention, however, then we calculate the auxiliary input `h_att` using our `attention_model` function and we splice that onto the input `x`. This augmented `x` is then used as input for the Decoder's `Recurrence`.\n",
    "\n",
    "Finally, we stabilize the output of the Decoder, put it through the final `Dense` layer `proj_out`, and label the output using the `Label` layer which allows for simple access to that layer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the s2s model\n",
    "def create_model(): # :: (history*, input*) -> logP(w)*\n",
    "    \n",
    "    # Embedding: (input*) --> embedded_input*\n",
    "    embed = C.layers.Embedding(embedding_dim, name='embed') if use_embedding else identity\n",
    "    \n",
    "    # Encoder: (input*) --> (h0, c0)\n",
    "    # Create multiple layers of LSTMs by passing the output of the i-th layer\n",
    "    # to the (i+1)th layer as its input\n",
    "    # Note: We go_backwards for the plain model, but forward for the attention model.\n",
    "    with C.layers.default_options(enable_self_stabilization=True, go_backwards=not use_attention):\n",
    "        LastRecurrence = C.layers.Fold if not use_attention else C.layers.Recurrence\n",
    "        encode = C.layers.Sequential([\n",
    "            embed,\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.For(range(num_layers-1), lambda:\n",
    "                C.layers.Recurrence(C.layers.LSTM(hidden_dim))),\n",
    "            LastRecurrence(C.layers.LSTM(hidden_dim), return_full_state=True),\n",
    "            (C.layers.Label('encoded_h'), C.layers.Label('encoded_c')),\n",
    "        ])\n",
    "\n",
    "    # Decoder: (history*, input*) --> unnormalized_word_logp*\n",
    "    # where history is one of these, delayed by 1 step and <s> prepended:\n",
    "    #  - training: labels\n",
    "    #  - testing:  its own output hardmax(z) (greedy decoder)\n",
    "    with C.layers.default_options(enable_self_stabilization=True):\n",
    "        # sub-layers\n",
    "        stab_in = C.layers.Stabilizer()\n",
    "        rec_blocks = [C.layers.LSTM(hidden_dim) for i in range(num_layers)]\n",
    "        stab_out = C.layers.Stabilizer()\n",
    "        proj_out = C.layers.Dense(label_vocab_dim, name='out_proj')\n",
    "        # attention model\n",
    "        if use_attention: # maps a decoder hidden state and all the encoder states into an augmented state\n",
    "            attention_model = C.layers.AttentionModel(attention_dim, \n",
    "                                                      attention_span, \n",
    "                                                      attention_axis, \n",
    "                                                      name='attention_model') # :: (h_enc*, h_dec) -> (h_dec augmented)\n",
    "        # layer function\n",
    "        @C.Function\n",
    "        def decode(history, input):\n",
    "            encoded_input = encode(input)\n",
    "            r = history\n",
    "            r = embed(r)\n",
    "            r = stab_in(r)\n",
    "            for i in range(num_layers):\n",
    "                rec_block = rec_blocks[i]   # LSTM(hidden_dim)  # :: (dh, dc, x) -> (h, c)\n",
    "                if use_attention:\n",
    "                    if i == 0:\n",
    "                        @C.Function\n",
    "                        def lstm_with_attention(dh, dc, x):\n",
    "                            h_att = attention_model(encoded_input.outputs[0], dh)\n",
    "                            x = C.splice(x, h_att)\n",
    "                            return rec_block(dh, dc, x)\n",
    "                        r = C.layers.Recurrence(lstm_with_attention)(r)\n",
    "                    else:\n",
    "                        r = C.layers.Recurrence(rec_block)(r)\n",
    "                else:\n",
    "                    # unlike Recurrence(), the RecurrenceFrom() layer takes the initial hidden state as a data input\n",
    "                    r = C.layers.RecurrenceFrom(rec_block)(*(encoded_input.outputs + (r,))) # :: h0, c0, r -> h                    \n",
    "            r = stab_out(r)\n",
    "            r = proj_out(r)\n",
    "            r = C.layers.Label('out_proj_out')(r)\n",
    "            return r\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that we defined above can be thought of as an \"abstract\" model that must first be wrapped to be used. In this case, we will use it first to create a \"training\" version of the model (where the history for the Decoder will be the ground-truth labels), and then we will use it to create a greedy \"decoding\" version of the model where the history for the Decoder will be the `hardmax` output of the network. Let's set up these model wrappers next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before starting training, we will define the training wrapper, the greedy decoding wrapper, and the criterion function used for training the model. Let's start with the training wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_train(s2smodel):\n",
    "    # model used in training (history is known from labels)\n",
    "    # note: the labels must NOT contain the initial <s>\n",
    "    @C.Function\n",
    "    def model_train(input, labels): # (input*, labels*) --> (word_logp*)\n",
    "\n",
    "        # The input to the decoder always starts with the special label sequence start token.\n",
    "        # Then, use the previous value of the label sequence (for training) or the output (for execution).\n",
    "        past_labels = C.layers.Delay(initial_state=sentence_start)(labels)\n",
    "        return s2smodel(past_labels, input)\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create the CNTK Function `model_train` again using the `@Function` decorator. This function takes the input sequence `input` and the output sequence `labels` as arguments. The `past_labels` are setup as the `history` for the model we created earlier by using the `Delay` layer. This will return the previous time-step value for the input `labels` with an `initial_state` of `sentence_start`. Therefore, if we give the labels `['a', 'b', 'c']`, then `past_labels` will contain `['<s>', 'a', 'b', 'c']` and then return our abstract base model called with the history `past_labels` and the input `input`.\n",
    "\n",
    "Let's go ahead and create the greedy decoding model wrapper now as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_greedy(s2smodel):\n",
    "    # model used in (greedy) decoding (history is decoder's own output)\n",
    "    @C.Function\n",
    "    @C.layers.Signature(InputSequence[C.layers.Tensor[input_vocab_dim]])\n",
    "    def model_greedy(input): # (input*) --> (word_sequence*)\n",
    "\n",
    "        # Decoding is an unfold() operation starting from sentence_start.\n",
    "        # We must transform s2smodel (history*, input* -> word_logp*) into a generator (history* -> output*)\n",
    "        # which holds 'input' in its closure.\n",
    "        unfold = C.layers.UnfoldFrom(lambda history: s2smodel(history, input) >> C.hardmax,\n",
    "                            # stop once sentence_end_index was max-scoring output\n",
    "                            until_predicate=lambda w: w[...,sentence_end_index],\n",
    "                            length_increase=length_increase)\n",
    "        \n",
    "        return unfold(initial_state=sentence_start, dynamic_axes_like=input)\n",
    "    return model_greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we create a new CNTK Function `model_greedy` which this time only takes a single argument. This is of course because when using the model at test time we don't have any labels -- it is the model's job to create them for us! In this case, we use the `UnfoldFrom` layer which runs the base model with the current `history` and funnels it into the `hardmax`. The `hardmax`'s output then becomes part of the `history` and we keep unfolding the `Recurrence` until the `sentence_end_index` has been reached. The maximum length of the output sequence (the maximum unfolding of the Decoder) is determined by a multiplier passed to `length_increase`. In this case we set `length_increase` to `1.5` above so the maximum length of each output sequence is 1.5x its input.\n",
    "\n",
    "The last thing we will do before setting up the training loop is define the function that will create the criterion function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model):\n",
    "    @C.Function\n",
    "    @C.layers.Signature(input=InputSequence[C.layers.Tensor[input_vocab_dim]], \n",
    "                        labels=LabelSequence[C.layers.Tensor[label_vocab_dim]])\n",
    "    def criterion(input, labels):\n",
    "        # criterion function must drop the <s> from the labels\n",
    "        postprocessed_labels = C.sequence.slice(labels, 1, 0) # <s> A B C </s> --> A B C </s>\n",
    "        z = model(input, postprocessed_labels)\n",
    "        ce = C.cross_entropy_with_softmax(z, postprocessed_labels)\n",
    "        errs = C.classification_error(z, postprocessed_labels)\n",
    "        return (ce, errs)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create the criterion function which drops the sequence-start symbol from our labels for us, runs the model with the given `input` and `labels`, and uses the output to compare to our ground truth. We use the loss function `cross_entropy_with_softmax` and get the `classification_error` which gives us the percent-error per-word of our generation accuracy. The CNTK Function `criterion` returns these values as a tuple and the Python function `create_criterion_function(model)` returns that CNTK Function.\n",
    "\n",
    "Now let's move on to creating the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_reader, valid_reader, vocab, i2w, s2smodel, max_epochs, epoch_size):\n",
    "\n",
    "    # create the training wrapper for the s2smodel, as well as the criterion function\n",
    "    model_train = create_model_train(s2smodel)\n",
    "    criterion = create_criterion_function(model_train)\n",
    "\n",
    "    # also wire in a greedy decoder so that we can properly log progress on a validation example\n",
    "    # This is not used for the actual training process.\n",
    "    model_greedy = create_model_greedy(s2smodel)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    minibatch_size = 72\n",
    "    lr = 0.001 if use_attention else 0.005\n",
    "    learner = C.fsadagrad(model_train.parameters,\n",
    "                          lr = C.learning_rate_schedule([lr]*2+[lr/2]*3+[lr/4], C.UnitType.sample, epoch_size),\n",
    "                          momentum = C.momentum_as_time_constant_schedule(1100),\n",
    "                          gradient_clipping_threshold_per_sample=2.3,\n",
    "                          gradient_clipping_with_truncation=True)\n",
    "    trainer = C.Trainer(None, criterion, learner)\n",
    "\n",
    "    # Get minibatches of sequences to train with and perform model training\n",
    "    total_samples = 0\n",
    "    mbs = 0\n",
    "    eval_freq = 100\n",
    "\n",
    "    # print out some useful training information\n",
    "    C.logging.log_number_of_parameters(model_train) ; print()\n",
    "    progress_printer = C.logging.ProgressPrinter(freq=30, tag='Training')    \n",
    "\n",
    "    # a hack to allow us to print sparse vectors\n",
    "    sparse_to_dense = create_sparse_to_dense(input_vocab_dim)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        while total_samples < (epoch+1) * epoch_size:\n",
    "            # get next minibatch of training data\n",
    "            mb_train = train_reader.next_minibatch(minibatch_size)\n",
    "            \n",
    "            # do the training\n",
    "            trainer.train_minibatch({criterion.arguments[0]: mb_train[train_reader.streams.features], \n",
    "                                     criterion.arguments[1]: mb_train[train_reader.streams.labels]})\n",
    "\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True) # log progress\n",
    "\n",
    "            # every N MBs evaluate on a test sequence to visually show how we're doing\n",
    "            if mbs % eval_freq == 0:\n",
    "                mb_valid = valid_reader.next_minibatch(1)\n",
    "\n",
    "                # run an eval on the decoder output model (i.e. don't use the groundtruth)\n",
    "                e = model_greedy(mb_valid[valid_reader.streams.features])\n",
    "                print(format_sequences(sparse_to_dense(mb_valid[valid_reader.streams.features]), i2w))\n",
    "                print(\"->\")\n",
    "                print(format_sequences(e, i2w))\n",
    "\n",
    "                # visualizing attention window\n",
    "                if use_attention:\n",
    "                    debug_attention(model_greedy, mb_valid[valid_reader.streams.features])\n",
    "\n",
    "            total_samples += mb_train[train_reader.streams.labels].num_samples\n",
    "            mbs += 1\n",
    "\n",
    "        # log a summary of the stats for the epoch\n",
    "        progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    # done: save the final model\n",
    "    model_path = \"model_%d.cmf\" % epoch\n",
    "    print(\"Saving final model to '%s'\" % model_path)\n",
    "    s2smodel.save(model_path)\n",
    "    print(\"%d epochs complete.\" % max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we created one version of the model for training (plus its associated criterion function) and one version of the model for evaluation. Normally this latter version would not be required but here we have done it so that we can periodically sample from the non-training model to visually understand how our model is converging by seeing the kinds of sequences that it generates as the training progresses.\n",
    "\n",
    "We then setup some standard variables required for the training loop. We set the `minibatch_size` (which refers to the total number of elements -- NOT sequences -- in a minibatch), the initial learning rate `lr`, we initialize a `learner` using the `adam_sgd` algorithm and a `learning_rate_schedule` that slowly reduces our learning rate. We make use of gradient clipping to help control exploding gradients, and we finally create our `Trainer` object `trainer`.\n",
    "\n",
    "We make use of CNTK's `ProgressPrinter` class which takes care of calculating average metrics per minibatch/epoch and we set it to update every 30 minibatches. And finally, before starting the training loop, we initialize a function called `sparse_to_dense` which we use to properly print out the input sequence data that we use for validation because it is sparse. That function is defined just below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy for printing the input sequence below. Currently needed because input is sparse.\n",
    "def create_sparse_to_dense(input_vocab_dim):\n",
    "    I = C.Constant(np.eye(input_vocab_dim))\n",
    "    @C.Function\n",
    "    @C.layers.Signature(InputSequence[C.layers.SparseTensor[input_vocab_dim]])\n",
    "    def no_op(input):\n",
    "        return C.times(input, I)\n",
    "    return no_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, we proceed much like many other CNTK networks. We request the next bunch of minibatch data, we perform our training, and we print our progress to the screen using the `progress_printer`. Where we diverge from the norm, however, is where we run an evaluation using our `model_greedy` version of the network and run a single sequence, \"ABADI\" through to see what the network is currently predicting.\n",
    "\n",
    "Another difference in the training loop is the optional attention window visualization. Calling the function `debug_attention` shows the weight that the Decoder put on each of the Encoder's hidden states for each of the output tokens that it generated. This function, along with the `format_sequences` function required to print the input/output sequences to the screen, are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a vocab and tensor, print the output\n",
    "def format_sequences(sequences, i2w):\n",
    "    return [\" \".join([i2w[np.argmax(w)] for w in s]) for s in sequences]\n",
    "\n",
    "# to help debug the attention window\n",
    "def debug_attention(model, input):\n",
    "    q = C.combine([model, model.attention_model.attention_weights])\n",
    "    #words, p = q(input) # Python 3\n",
    "    words_p = q(input)\n",
    "    words = words_p[0]\n",
    "    p     = words_p[1]\n",
    "    seq_len = words[0].shape[attention_axis-1]\n",
    "    span = 7 #attention_span  #7 # test sentence is 7 tokens long\n",
    "    p_sq = np.squeeze(p[0][:seq_len,:span,0,:]) # (batch, len, attention_span, 1, vector_dim)\n",
    "    opts = np.get_printoptions()\n",
    "    np.set_printoptions(precision=5)\n",
    "    print(p_sq)\n",
    "    np.set_printoptions(**opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training our network for a small part of an epoch. In particular, we'll run through 25,000 tokens (about 3% of one epoch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 7561400 parameters in 29 parameter tensors.\n",
      "\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['</s>']\n",
      "[ 0.1434   0.14277  0.14289  0.14291  0.14294  0.14257  0.14252]\n",
      " Minibatch[   1-  30]: loss = 4.147692 * 1601, metric = 87.51% * 1601;\n",
      " Minibatch[  31-  60]: loss = 3.665605 * 1601, metric = 86.26% * 1601;\n",
      " Minibatch[  61-  90]: loss = 3.329608 * 1548, metric = 89.34% * 1548;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~N ~N </s>']\n",
      "[[ 0.14386  0.14317  0.1432   0.14308  0.14289  0.14219  0.14161]\n",
      " [ 0.14386  0.14317  0.1432   0.14308  0.14289  0.14219  0.14161]\n",
      " [ 0.14386  0.14317  0.1432   0.14308  0.14289  0.14219  0.14161]]\n",
      " Minibatch[  91- 120]: loss = 3.240603 * 1567, metric = 86.85% * 1567;\n",
      " Minibatch[ 121- 150]: loss = 3.217582 * 1580, metric = 83.54% * 1580;\n",
      " Minibatch[ 151- 180]: loss = 3.212167 * 1544, metric = 83.94% * 1544;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~R ~R ~AH ~AH ~AH </s>']\n",
      "[[ 0.1439   0.14321  0.14323  0.14308  0.14287  0.14216  0.14156]\n",
      " [ 0.1439   0.14321  0.14322  0.14308  0.14287  0.14216  0.14156]\n",
      " [ 0.1439   0.14321  0.14322  0.14308  0.14287  0.14216  0.14156]\n",
      " [ 0.1439   0.14321  0.14322  0.14308  0.14287  0.14216  0.14156]\n",
      " [ 0.1439   0.14321  0.14322  0.14308  0.14287  0.14216  0.14156]\n",
      " [ 0.1439   0.14321  0.14322  0.14308  0.14287  0.14216  0.14156]]\n",
      " Minibatch[ 181- 210]: loss = 3.143626 * 1565, metric = 82.30% * 1565;\n",
      " Minibatch[ 211- 240]: loss = 3.186273 * 1583, metric = 83.39% * 1583;\n",
      " Minibatch[ 241- 270]: loss = 3.128010 * 1562, metric = 83.03% * 1562;\n",
      " Minibatch[ 271- 300]: loss = 3.152663 * 1551, metric = 83.69% * 1551;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~R ~R ~R ~AH </s>']\n",
      "[[ 0.14402  0.14333  0.1433   0.14312  0.14284  0.14205  0.14134]\n",
      " [ 0.14402  0.14333  0.1433   0.14312  0.14284  0.14205  0.14134]\n",
      " [ 0.14402  0.14333  0.1433   0.14312  0.14284  0.14205  0.14134]\n",
      " [ 0.14402  0.14333  0.1433   0.14312  0.14284  0.14205  0.14134]\n",
      " [ 0.14402  0.14332  0.1433   0.14312  0.14284  0.14205  0.14134]]\n",
      " Minibatch[ 301- 330]: loss = 3.136208 * 1575, metric = 82.92% * 1575;\n",
      " Minibatch[ 331- 360]: loss = 3.101594 * 1569, metric = 82.79% * 1569;\n",
      " Minibatch[ 361- 390]: loss = 3.110577 * 1567, metric = 82.26% * 1567;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~K ~R ~R ~AH ~AH </s>']\n",
      "[[ 0.14417  0.14348  0.1434   0.14316  0.14278  0.14191  0.1411 ]\n",
      " [ 0.14417  0.14348  0.14339  0.14316  0.14279  0.14191  0.14111]\n",
      " [ 0.14417  0.14348  0.1434   0.14316  0.14279  0.14191  0.1411 ]\n",
      " [ 0.14417  0.14348  0.14339  0.14316  0.14279  0.14191  0.14111]\n",
      " [ 0.14417  0.14347  0.14339  0.14316  0.14279  0.14191  0.14111]\n",
      " [ 0.14417  0.14348  0.14339  0.14316  0.14279  0.14191  0.14111]]\n",
      " Minibatch[ 391- 420]: loss = 3.126911 * 1601, metric = 82.26% * 1601;\n",
      "Finished Epoch[1]: [Training] loss = 3.279741 * 22067, metric = 84.28% * 22067 40.852s (540.2 samples/s);\n",
      "Saving final model to 'model_0.cmf'\n",
      "1 epochs complete.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "train(train_reader, valid_reader, vocab, i2w, model, max_epochs=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, while the loss has come down quite a ways, the output sequence is still quite a ways off from what we expect. Uncomment the code below to run for a full epoch (notice that we switch the `epoch_size` parameter to the actual size of the training data) and by the end of the first epoch you will already see a very good grapheme-to-phoneme translation model running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below to train the model for a full epoch\n",
    "#train(train_reader, valid_reader, vocab, i2w, model, max_epochs=1, epoch_size=908241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network\n",
    "\n",
    "Now that we've trained a sequence-to-sequence network for graphme-to-phoneme translation, there are two important things we should do with it. First, we should test its accuracy on a held-out test set. Then, we should try it out in an interactive environment so that we can put in our own input sequences and see what the model predicts. Let's start by determining the test string error rate.\n",
    "\n",
    "At the end of training, we saved the model using the line `s2smodel.save(model_path)`. Therefore, to test it, we will need to first `load` that model and then run some test data through it. Let's `load` the model, then create a reader configured to access our testing data. Note that we pass `False` to the `create_reader` function this time to denote that we are in testing mode so we should only pass over the data a single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the model for epoch 0\n",
    "model_path = \"model_0.cmf\"\n",
    "model = C.Function.load(model_path)\n",
    "\n",
    "# create a reader pointing at our testing data\n",
    "test_reader = create_reader(dataPath['testing'], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define our testing function. We pass the `reader`, the learned `s2smodel`, and the vocabulary map `i2w` so that we can directly compare the model's predictions to the test set labels. We loop over the test set, evaluate the model on minibatches of size 512 for efficiency, and keep track of the error rate. Note that below we test *per-sequence*. This means that every single token in a generated sequence must match the tokens in the label for that sequence to be considered as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This decodes the test set and counts the string error rate.\n",
    "def evaluate_decoding(reader, s2smodel, i2w):\n",
    "    \n",
    "    model_decoding = create_model_greedy(s2smodel) # wrap the greedy decoder around the model\n",
    "\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation')\n",
    "\n",
    "    sparse_to_dense = create_sparse_to_dense(input_vocab_dim)\n",
    "\n",
    "    minibatch_size = 512\n",
    "    num_total = 0\n",
    "    num_wrong = 0\n",
    "    while True:\n",
    "        mb = reader.next_minibatch(minibatch_size)\n",
    "        if not mb: # finish when end of test set reached\n",
    "            break\n",
    "        e = model_decoding(mb[reader.streams.features])\n",
    "        outputs = format_sequences(e, i2w)\n",
    "        labels  = format_sequences(sparse_to_dense(mb[reader.streams.labels]), i2w)\n",
    "        # prepend sentence start for comparison\n",
    "        outputs = [\"<s> \" + output for output in outputs]\n",
    "\n",
    "        num_total += len(outputs)\n",
    "        num_wrong += sum([label != output for output, label in zip(outputs, labels)])\n",
    "        \n",
    "    rate = num_wrong / num_total\n",
    "    print(\"string error rate of {:.1f}% in {} samples\".format(100 * rate, num_total))\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the decoding using the above function. If you use the version of the model we trained above with just a small 50000 sample of the training data, you will get an error rate of 100% because we cannot possibly get every single token correct with such a small amount of training. However, if you uncommented the training line above that trains the network for a full epoch, you should have ended up with a much-improved model that showed approximately the following training statistics:\n",
    "\n",
    "```\n",
    "Finished Epoch[1 of 300]: [Training] loss = 0.878420 * 799303, metric = 26.23% * 799303 1755.985s (455.2 samples/s);\n",
    "```\n",
    "\n",
    "Now let's evaluate the model's test set performance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string error rate of 100.0% in 12855 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the string error rate\n",
    "evaluate_decoding(test_reader, model, i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you did not run the training for the full first epoch, the output above will be a `1.0` meaning 100% string error rate. If, however, you uncommented the line to perform training for a full epoch, you should get an output of `0.569`. A string error rate of `56.9` is actually not bad for a single pass over the data. Let's now modify the above `evaluate_decoding` function to output the per-phoneme error rate. This means that we are calculating the error at a higher precision and also makes things easier in some sense because with the string error rate we could have every phoneme correct but one in each example and still end up with a 100% error rate. Here is the modified version of that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This decodes the test set and counts the string error rate.\n",
    "def evaluate_decoding(reader, s2smodel, i2w):\n",
    "    \n",
    "    model_decoding = create_model_greedy(s2smodel) # wrap the greedy decoder around the model\n",
    "\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation')\n",
    "\n",
    "    sparse_to_dense = create_sparse_to_dense(input_vocab_dim)\n",
    "\n",
    "    minibatch_size = 512\n",
    "    num_total = 0\n",
    "    num_wrong = 0\n",
    "    while True:\n",
    "        mb = reader.next_minibatch(minibatch_size)\n",
    "        if not mb: # finish when end of test set reached\n",
    "            break\n",
    "        e = model_decoding(mb[reader.streams.features])\n",
    "        outputs = format_sequences(e, i2w)\n",
    "        labels  = format_sequences(sparse_to_dense(mb[reader.streams.labels]), i2w)\n",
    "        # prepend sentence start for comparison\n",
    "        outputs = [\"<s> \" + output for output in outputs]\n",
    "        \n",
    "        for s in range(len(labels)):\n",
    "            for w in range(len(labels[s])):\n",
    "                num_total += 1\n",
    "                if w < len(outputs[s]): # in case the prediction is longer than the label\n",
    "                    if outputs[s][w] != labels[s][w]:\n",
    "                        num_wrong += 1\n",
    "                \n",
    "    rate = num_wrong / num_total\n",
    "    print(\"{:.1f}\".format(100 * rate))\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45012815036309267"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the phoneme error rate\n",
    "test_reader = create_reader(dataPath['testing'], False)\n",
    "evaluate_decoding(test_reader, model, i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using the model that was trained for one full epoch, then you should get a phoneme error rate of around 10%. Not bad! This means that for each of the 383,294 phonemes in the test set, our model predicted nearly 90% of them correctly (if you used the quickly-trained version of the model then you will get an error rate of around 45%). Now, let's work with an interactive session where we can input our own input sequences and see how the model predicts their pronunciation (i.e. phonemes). Additionally, we will visualize the Decoder's attention for these samples to see which graphemes in the input it deemed to be important for each phoneme that it produces. Note that in the examples below the results will only be good if you use a model that has been trained for at least one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive session\n",
    "\n",
    "Here we will write an interactive function to make it easy to interact with the trained model and try out your own input sequences that do not appear in the test set. Please note that the results will be very poor if you just use the model that was trained for a very short amount of time. The model we used just above that was trained for one epoch does a good job, and if you have the time and patience to train the model for a full 30 epochs, it will perform very nicely.\n",
    "\n",
    "We will first import some graphics libraries that make the attention visualization possible and then we will define the `translate` function that takes a numpy-based representation of the input and runs our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports required for showing the attention weight heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def translate(tokens, model_decoding, vocab, i2w, show_attention=False):\n",
    "\n",
    "    vdict = {v:i for i,v in enumerate(vocab)}\n",
    "    try:\n",
    "        w = [vdict[\"<s>\"]] + [vdict[c] for c in tokens] + [vdict[\"</s>\"]]\n",
    "    except:\n",
    "        print('Input contains an unexpected token.')\n",
    "        return []\n",
    "\n",
    "    # convert to one_hot\n",
    "    query = C.Value.one_hot([w], len(vdict))\n",
    "    pred = model_decoding(query)\n",
    "    pred = pred[0] # first sequence (we only have one) -> [len, vocab size]\n",
    "    if use_attention:\n",
    "        pred = pred[:,0,0,:] # attention has extra dimensions\n",
    "\n",
    "    # print out translation and stop at the sequence-end tag\n",
    "    prediction = np.argmax(pred, axis=-1)\n",
    "    translation = [i2w[i] for i in prediction]\n",
    "    \n",
    "    # show attention window (requires matplotlib, seaborn, and pandas)\n",
    "    if use_attention and show_attention:    \n",
    "        q = C.combine([model_decoding.attention_model.attention_weights])\n",
    "        att_value = q(query)\n",
    "\n",
    "        # get the attention data up to the length of the output (subset of the full window)\n",
    "        att_value = att_value[0][0:len(prediction),0:len(w),0,0] # -> (len, span)\n",
    "\n",
    "        # set up the actual words/letters for the heatmap axis labels\n",
    "        columns = [i2w[ww] for ww in prediction]\n",
    "        index = [i2w[ww] for ww in w]\n",
    "\n",
    "        dframe = pd.DataFrame(data=np.fliplr(att_value.T), columns=columns, index=index)\n",
    "        sns.heatmap(dframe)\n",
    "        plt.show()\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `translate` function above takes a list of letters input by the user as `tokens`, the greedy decoding version of our model `model_decoding`, the vocabulary `vocab`, a map of index to vocab `i2w`, and the `show_attention` option which determines if we will visualize the attention vectors or not.\n",
    "\n",
    "We convert our input into a `one_hot` representation, run it through the model with `model_decoding(query)` and, since each prediction is actually a probability distribution over the entire vocabulary, we take the `argmax` to get the most probable token for each step.\n",
    "\n",
    "To visualize the attention window, we use `combine` to turn the `attention_weights` into a CNTK Function that takes the inputs that we expect. This way, when we run the function `q`, the output will be the values of the `attention_weights`. We do some data manipulation to get this data into the format that `sns` expects, and we show the visualization.\n",
    "\n",
    "Finally, we need to write the user-interaction loop which allows a user to enter multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interactive_session(s2smodel, vocab, i2w, show_attention=False):\n",
    "\n",
    "    model_decoding = create_model_greedy(s2smodel) # wrap the greedy decoder around the model\n",
    "\n",
    "    import sys\n",
    "\n",
    "    print('Enter one or more words to see their phonetic transcription.')\n",
    "    while True:\n",
    "        if isTest(): # Testing a prefilled text for routine testing\n",
    "            line = \"psychology\"\n",
    "        else:    \n",
    "            line = input(\"> \")\n",
    "        if line.lower() == \"quit\":\n",
    "            break\n",
    "        # tokenize. Our task is letter to sound.\n",
    "        out_line = []\n",
    "        for word in line.split():\n",
    "            in_tokens = [c.upper() for c in word]\n",
    "            out_tokens = translate(in_tokens, model_decoding, vocab, i2w, show_attention=True)\n",
    "            out_line.extend(out_tokens)\n",
    "        out_line = [\" \" if tok == '</s>' else tok[1:] for tok in out_line]\n",
    "        print(\"=\", \" \".join(out_line))\n",
    "        sys.stdout.flush()\n",
    "        if isTest(): #If test environment we will test the translation only once\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function simply creates a greedy decoder around our model and then continually asks the user for an input which we pass to our `translate` function. Visualizations of the attention will continue being appended to the notebook until you exit the loop by typing `quit`. Please uncomment the following line to try out the interaction session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter one or more words to see their phonetic transcription.\n",
      "> eleph\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAFSCAYAAABCJY7TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHztJREFUeJzt3X9UVHUe//HXgIJBGpKCJ6iOR1M5uNZoK0talqL90Mo9\nIAyZoK3Wblkm5Qn7QVaaqccljUhL3cRTccqWOZnbtlux1JopR1tFt8xE+C4VEVRLyxrTxv3+0bf5\nOkkz4/UOMN7nwzPncH/O+2bHF+/P/cwdh2EYhgAAwEmJ6O4CAAAIRwQoAAAmEKAAAJhAgAIAYAIB\nCgCACQQoAAAm9Ar1G4w6f0Ko3wIAEKT99VUhO/ep/HsfyrpChQ4UAAATQt6BAgDsweFwdHcJXYoA\nBQBYwuGw16Cmva4WAACL0IECACwRIYZwAQA4aXa7B8oQLgAAJtCBAgAsEWGzSUQEKADAEgzhAgCA\ngOhAAQCWcNhsFi4dKADAEhGOCNOvQJYvXy6Xy6Xc3FzV1NT4bPN4PCosLFRmZuYJx7W3t2vy5Mly\nu90+69955x2NGDHCu1xcXKzc3Fy5XC5t2LAhuOsNai8AALpJdXW16uvrVV5erqVLl2rZsmU+21eu\nXKmUlJRO78GWlpYqLi7OZ53H49HTTz+thIQESdLhw4e1a9cuvfDCC3rhhRf0xz/+US0tLQHrIkAB\nAJZwOBymX/7s3LlTGRkZkqQhQ4aotbVVbW1t3u0FBQXe7cerra1VbW2tJkzw/ZaYdevW6cYbb1Tv\n3r0lSX379pXH45HH49G3336ryMhI9enTJ+D1Bh2gra2t2rt3b7C7AwBsJsLhMP3yp7m5WfHx8d7l\n/v37q7m52bscExPT6XErVqxQYWGhz7qjR4/q0KFDuvLKK2UYhiRp0KBBuuqqqzRx4kRNmjRJLpdL\nsbGxga834B7/z+OPP64HHnhAe/bsCfYQAAAs92Pw+eN2u+V0OpWUlOSz/rHHHjshVP/1r3/pjTfe\n0FtvvaW//OUveuGFF/Tll18GfI+gZuF++eWX+uc//6k1a9bo97//vcaMGRPMYQAAG3GE6K5gQkKC\nT8fZ1NSkgQMH+j2mqqpKDQ0NqqysVGNjo6KjoyX90IEuWrRIhmHoiy++0KxZs5Sbm6tRo0YpKipK\nUVFRGj58uA4fPqy0tDS/7xFUgJaVlSk/P19Dhw5Vr1699PHHH2vo0KHBHAoAsIlQPUhh3LhxKikp\nUXZ2tg4ePKjExMQThm0Nw/DpTIuLi70/l5SUKDk5WdOnT9f06dO96ydOnKgtW7bo4MGDKisrkyR9\n9913+uijj3TuuecGrCtggLa1tWnHjh268847JUnz5s3Thg0b9NhjjwU8OQAAp8rpdCo1NVUul0uR\nkZEqKipSRUWF+vbtq4yMDC1YsECNjY2qq6tTXl6ecnJyNHXq1IDn/THwU1NTNX78eLlcLjkcDmVn\nZ+ucc84JfLwRYDB57969am1t1eWXX+5d9+STT+qWW25Rr16BG9hR508IuA8AoGvsr68K2bkvT5ke\neKef8bcP3IF36mECDliPHj1aF198sY4ePSpJ2r17t2JjY9Xa2hry4gAA4cNxCn/CUVB3fO+88041\nNTXp8OHDWrFiheLj47V48eJQ1wYAQI8VVIB6PB6lpaXptdde0+zZs3Xdddepvb091LUBAMJIKB/l\n1xMFHaCvvPKKtm/friuuuEINDQ365ptvQl0bACCMhOpJRD1VwACtra1VYWGh9u/fryVLlujMM89U\nVVWVbrvttq6oDwCAHingNNo+ffpo2bJl2rhxoyTp888/11tvveVdBgBAUsBH8p1uAnag55xzjgYM\nGKD9+/dLkjZt2qT8/PyQFwYACC/Mwu3E3LlztX79en399dfat2+fLrvsslDXBQBAjxbUo/wuuOAC\ndXR0qKioSDNnzgx1TQCAMBSus2nNCvpqFy5cqJSUlKAejwQAsB+7zcINqgOVpGHDhmnYsGGhrAUA\ngLARdIACAOCP3WbhEqAAAEuE62xas+x1xxcAAIvQgQIALBGuk4HMIkABAJaw2z1QhnABADCBDhQA\nYAm7TSIiQAEAluBJRAAAICA6UACAJZiFCwCACczCBQAAAYW8Ax1/vjPUbwEA6AGYhQsAgAkM4QIA\ngIDoQAEAlmAWLgAAJjCECwAAAqIDBQBYglm4AACYwBAuAAAIiA4UAGAJZuECAGACQ7gAACAgOlAA\ngCWYhQsAgAkM4QIAgIDoQAEAlmAWLgAAJjCECwAAAqIDBQBYwm5DuH470NbW1p/dVlNTY3kxAIDw\n5TiFP+HIb4DOnz/fZ/nBBx/0/rxq1arQVAQAQBjwO4RrGIbPcm1t7c9uAwDYW0R4NpKm+Q1Qf+PZ\ndhvrBgD4Z7dcOKlZuHb7jwMAwM/x24EeOHBAWVlZkn4Ysj169KiysrJkGIbq6uq6oj4AQJiw2+dA\n/Qbotm3buqoOAECYs9sopd8ATUpK6qo6AAAIKzxIAQBgiYgw/TynWQQoAMASdhvC5Vm4AACYQAcK\nALAEs3ABADDBZvnJEC4AAGbQgQIALGG3IVw6UACAJUL5dWbLly+Xy+VSbm7uCV+n6fF4VFhYqMzM\nzBOOa29v1+TJk+V2u33Wv/POOxoxYoR3+ZVXXlFWVpZycnK0devWoK6XDhQA0KNVV1ervr5e5eXl\nOnLkiO677z6Vl5d7t69cuVIpKSn6+OOPTzi2tLRUcXFxPus8Ho+efvppJSQkSJKOHTum0tJSvfzy\ny+rVq5eysrI0ZcoU9evXz29ddKAAAEs4HA7TL3927typjIwMSdKQIUPU2tqqtrY27/aCggLv9uPV\n1taqtrZWEyZM8Fm/bt063Xjjjerdu7ckad++fRo1apRiY2MVHR2t0aNHa+/evQGvlwAFAFgiwuEw\n/fKnublZ8fHx3uX+/furubnZuxwTE9PpcStWrFBhYaHPuqNHj+rQoUO68sorf/b88fHx+uKLLwJe\nL0O4AABLdNUcIsMwAu7jdrvldDpPeKb7Y489pvvvv9/veYI5v0SAAgB6uISEBJ+Os6mpSQMHDvR7\nTFVVlRoaGlRZWanGxkZFR0dL+qEDXbRokQzD0BdffKFZs2bpjjvuUGVlpffYzz//XE6nM2BdBCgA\nwBKh+hjLuHHjVFJSouzsbB08eFCJiYknDNsahuHTORYXF3t/LikpUXJysqZPn67p06d710+cOFFb\ntmxRe3u77r//fv3nP/+Rw+HQ+++/r/vuuy9gXQQoAKBHczqdSk1NlcvlUmRkpIqKilRRUaG+ffsq\nIyNDCxYsUGNjo+rq6pSXl6ecnBxNnTo14Hl/nLwUHR2tu+66SzfddJMiIiJ0++2368wzzwx8vBHs\nYK9Jt152ZyhPDwA4CaVvPx6ycy+55n7zx/5pqYWVdA06UACAJez2JKKQB2jmuBGBdwIAhD2b5Sef\nAwUAwAyGcAEAlgj0RKHTDR0oAAAm0IECACzBJCIAAEywWX4SoAAAa9itA+UeKAAAJtCBAgAs4RAd\nKAAACIAOFABgCbt9DpQABQBYIsJe+UmAAgCsYbcOlHugAACYQAcKALCE3TpQAhQAYAm73QNlCBcA\nABPoQAEAlmAIFwAAE2yWnwzhAgBghukArampsbIOAECYi3A4TL/CkekAXbVqlZV1AADCnOMU/oQj\n0wFqGIaVdQAAEFZMTyKy22wrAIB/dosFvwGamZnZaVAahqG6urpQ1QQACEPhei/TLL8Bunbt2q6q\nAwCAsOI3QJOSkrqqDgBAmLPbrT0epAAAsITN8pMHKQAAYAYdKADAEgzhAgBgAl9nBgAAAqIDBQBY\ngiFcAABMsFl+MoQLAIAZdKAAAEvwKD8AAEyw2z1QhnABADCBDhQAYAmbNaAEKADAGgzhAgCAgOhA\nAQCWsFkDSoACAKxht4+xMIQLAIAJIe9Az/9FQqjfAgDQA9isAWUIFwBgDWbhAgCAgOhAAQCWsFkD\nSoACAKzBEC4AAAiIDhQAYAmbNaAEKADAGjxIAQAABEQHCgCwhM0aUAIUAGANZuECAICA6EABAJaw\nWQNKBwoAsIbD4TD9CmT58uVyuVzKzc1VTU2NzzaPx6PCwkJlZmaecFx7e7smT54st9stSXr//fd1\nww03KC8vT/PmzdNXX30lSfrTn/6kGTNmyOVyqbi4OKjrJUABAD1adXW16uvrVV5erqVLl2rZsmU+\n21euXKmUlJROg7i0tFRxcXHe5c2bN2vVqlUqKyvThRdeqJdeeknffvutVq9erbKyMpWXl2vnzp06\ncuRIwLoIUACAJRwO8y9/du7cqYyMDEnSkCFD1Nraqra2Nu/2goIC7/bj1dbWqra2VhMmTPCue/zx\nx5WUlCTDMNTU1KTExET16dNH27Zt0xlnnCFJiouL09dffx3weglQAIAlQjWE29zcrPj4eO9y//79\n1dzc7F2OiYnp9LgVK1aosLDwhPXvvPOOrrrqKrW0tOj666/3OcehQ4f06aef6qKLLgp4vQQoACCs\nGIYRcB+32y2n06mkpKQTjrn00kv1+uuva/DgwVq/fr13fV1dne6++26tXr1akZGRAd+DWbgAAEuE\nahZuQkKCT8fZ1NSkgQMH+j2mqqpKDQ0NqqysVGNjo6KjozVo0CC1tbV5h3unTJmiJ598UpLU2Nio\n22+/XatWrdLw4cODqstvgLa2tqpfv36dbqupqdEvfvGLoN4EAHD6C9WDFMaNG6eSkhJlZ2fr4MGD\nSkxMPGHY1jAMny7z+Jm0JSUlSk5OVnp6uq6//nolJydrxIgR2r9/vwYPHixJuu+++/Tggw9qxIgR\nQdflN0Dnz5+vsrIy7/KDDz6ohx56SJK8s5gAAAglp9Op1NRUuVwuRUZGqqioSBUVFerbt68yMjK0\nYMECNTY2qq6uTnl5ecrJydHUqVM7Pdejjz6qJUuWqHfv3oqOjtbKlStVV1envXv3au3atTIMQw6H\nQ3PmzNEVV1zhty6/AfrTceba2tqf3QYAsLdQPkihoKDAZ/n4YdY1a9b4PXb+/Pnen1NTU1VeXu6z\nPT4+Xu+///5J1+Q3QP2143Z75iEAwD+7fZ3ZSU0iIjQBAD/HbhHhN0APHDigrKwsST8M2R49elRZ\nWVkyDEN1dXVdUR8AAD2S3wDdtm1bV9UBAAhzdhul9BugP34AFQAA+OJBCgAAS9isASVAAQDWcETY\nK0EJUACAJezWgfIweQAATKADBQBYwm6zcOlAAQAwgQ4UAGAJmzWgBCgAwBp2G8IlQAEAlrBZfnIP\nFAAAM+hAAQDWsFkLSgcKAIAJdKAAAEswiQgAABNslp8EKADAGnZ7mDz3QAEAMIEOFABgCYZwLdb3\nvLND/RboLobR3RUgVOz2LyFgAh0oAMASzMIFAMAEm+UnAQoAsIbdOlBm4QIAYAIdKADAEjZrQOlA\nAQAwgw4UAGAJu90DJUABANaw2ZgmAQoAsITdOlCb/b4AAIA16EABAJawWQNKBwoAgBl0oAAAS9jt\nHigBCgCwhM3ykwAFAFjEZgnKPVAAAEygAwUAWMIRQQcKAAACCBigHo9Hn3zyiTo6OrqiHgBAmHI4\nzL/Ckd8AfeONN3TVVVepoKBAV199tfbt29dVdQEAwozD4TD9Ckd+74Fu2LBBFRUVOuuss9TQ0KAl\nS5Zow4YNXVUbACCMhGkOmua3A+3du7fOOussSVJycrLa29u7pCgAAHo6vx3oT9vqcG2zAQBdwGYZ\n4TdADxw4oKysLEmSYRg6evSosrKyZBiGHA6Htm7d2iVFAgDQ0/gN0G3btnVVHQCAMGe3z4H6DdCk\npKSuqgMAEOZsNoLLk4gAABaxWYLyJCIAAEygAwUAWMJmDSgdKAAAZtCBAgAswSxcAABMsNvDdghQ\nAIA17JWf3AMFAMAMAhQAYIlQfp3Z8uXL5XK5lJubq5qaGp9tHo9HhYWFyszMPOG49vZ2TZ48WW63\nW5L02Wefac6cOZo1a5ZuuukmtbS0+OxfUFCgxYsXB3W9BCgAoEerrq5WfX29ysvLtXTpUi1btsxn\n+8qVK5WSktJpEJeWliouLs67vGbNGrlcLm3ZskWTJk3Spk2bvNt27NihhoaGoOsiQAEAlghVB7pz\n505lZGRIkoYMGaLW1la1tbV5txcUFHi3H6+2tla1tbWaMGGCd92SJUs0ZcoUSVJ8fLz+/e9/S/qh\ni123bp1+97vfBX29BCgAwBoRp/Dyo7m5WfHx8d7l/v37q7m52bscExPT6XErVqxQYWGhz7o+ffrI\n4XCoo6NDzz//vK699lpJ0tNPP63c3FzFxsae1OUCAHDKQnkP9HiGYQTcx+12y+l0er8U5fhjOjo6\ntGjRIqWnpystLU319fU6cOCArrnmmqDO/SM+xgIA6NESEhJ8Os6mpiYNHDjQ7zFVVVVqaGhQZWWl\nGhsbFR0drUGDBik9PV2LFy/W4MGDdeutt0qS/va3v+mzzz6Ty+XSN998o6+++kobN27Ub37zG7/v\nQYACACwRqgcpjBs3TiUlJcrOztbBgweVmJh4wrCtYRg+3WNxcbH355KSEiUnJys9PV2vvPKKoqKi\nNH/+fO/2/Px85efnS5J2796tioqKgOEpEaAAgB7O6XQqNTVVLpdLkZGRKioqUkVFhfr27auMjAwt\nWLBAjY2NqqurU15ennJycjR16tROz/X888/L4/Fo1qxZcjgcGjp0qIqKikzV5TBOZsDXhM//XhXK\n06M7hfZ/HXQnmz2SzU4Sx08IvJNJR8orTB87xPVrCyvpGnSgMI9/ZHEasNvzW0OJh8kDAGCGzX4Z\n4WMsAACYQAcKALCEzRpQOlAAAMygAwUAWMJuE7IIUACANZiFCwDAybNbB8o9UAAATKADBQBYw14N\nKB0oAABm0IECACxht3ugBCgAwBI8CxcAADPoQAEAOHl2G8JlEhEAACYQoAAAmMAQLgDAGvYawSVA\nAQDWYBbucZ577jm/B8+cOdPSYgAAYcxmk4j8BuhXX33l/bmiokK//vWvQ14QACA82W0Wrt8AnT9/\nvvfnXbt2+SwDAGBnQc/CtdtvFgAA+MMkIgCANZhE9P9lZmbK4XDIMAwdPXpUWVlZkiTDMORwOLR1\n69YuKRIA0PPZbaTSb4CuXbu2q+oAAIQ7e+Wn/wBNSkrqqjoAAGHObh0oj/IDAMAEAhQAABOYhQsA\nsAazcAEAOHl2uwdKgAIArEGAAgBw8uzWgTKJCAAAEwhQAABMYAgXAGANZuECAHDy7HYPlAAFAFiD\nAAUA4OQ5bDaEyyQiAABMIEABADCBIVwAgDW4BwoAwMljFq7F+o8aGeq3AAD0BAQoAAAnj1m4AAAg\nIAIUAAATGMIFAFiDe6AAAJhAgAIAcPL4GAsAAGYwCxcAAARCgAIAYAJDuAAASzgc9urJCFAAgDWY\nRAQAwMljFi4AAGYwCxcAAAQSdIA+88wzSktLU0NDQyjrAQDgBMuXL5fL5VJubq5qamp8tnk8HhUW\nFiozM/OE49rb2zV58mS53W7vurKyMo0cOVLHjh3zrvvwww+VmZmprKwslZaWBlVTUAHq8Xj02muv\n6dFHH9WmTZuCOjEAwF4cDofplz/V1dWqr69XeXm5li5dqmXLlvlsX7lypVJSUjo9T2lpqeLi4rzL\nbrdbLS0tSkhI8NmvqKhIy5Yt09atW3XkyBG1t7cHvN6gAtTtduuaa67RpEmTdOjQIX355ZfBHAYA\nsBOHw/zLj507dyojI0OSNGTIELW2tqqtrc27vaCgwLv9eLW1taqtrdWECRO866ZMmaKFCxf67NfS\n0qJjx45pxIgRkqTVq1crOjo64OUGDNCOjg5t3bpVN9xwgyQpPz9fW7ZsCXhiAIDNOCLMv/xobm5W\nfHy8d7l///5qbm72LsfExHR63IoVK1RYWOizrrN9P/nkE/Xr10+LFy/WDTfcoM2bNwd1uQFn4b73\n3nvKyMjwvumPY8n/+9//1KsXk3gBAD9wdNEsXMMwAu7jdrvldDqVlJQU8BjDMPTJJ5/oqaeeUlRU\nlHJycjR+/HgNGTLE73sETMBLLrlEl1xyiTwej6KiotTa2qrbb7+d8AQAdImEhASfjrOpqUkDBw70\ne0xVVZUaGhpUWVmpxsZGRUdHa9CgQUpPT5fk+5nVs88+W0OHDlW/fv0kSWPGjNHhw4dPPUAl6ZFH\nHtHIkSN12WWXKT8/X06nUw6HQw8//HAwhwMAYNq4ceNUUlKi7OxsHTx4UImJiScMxRqG4dNlFhcX\ne38uKSlRcnKyNzx/3P9HycnJamtrU2trq84880x98MEHysnJCVhXUAH64Ycf6oEHHtDmzZuVlZWl\n2bNna86cOcEcCgCwixA9icjpdCo1NVUul0uRkZEqKipSRUWF+vbtq4yMDC1YsECNjY2qq6tTXl6e\ncnJyNHXq1E7PtW7dOu3YsUMtLS2aN2+eLrroIt19991avHix5s6dq4iICI0fP17Dhw8PWJfDCGIw\necaMGSopKdGtt96qJ598UgMGDFBOTo5efvnlgG/gaW0JuA8AoGtE9Ts7ZOf+T/1Hpo898/xhFlbS\nNYLqQGfOnKl58+Zp2rRpGjRokIqLi3XllVeGujYAQDix2bex+O1AV69erbS0NI0ePdpnvNkwjKAf\nGkwHCgA9Ryg70LaGI6aPjU32P2GnJ/LbgV5++eXatWuX/vCHP+i7777TqFGjNHbsWI0ZM0axsbFd\nVSMAAD1OwHugR44cUUREhJKSkrR//3699957WrdunUaOHKny8vKAb0AHCgA9Bx2odQIOWJ9xxhla\nunSpoqKidPHFF2vGjBlKS0tTWVlZV9QHAAgXIXqUX08VMEDPOeccDRgwQPv375ckbdq0SbNnz1ZU\nVFTIiwMAhI9QPUy+pwpqytTcuXO1fv16ff3119q3b58uvfTSUNcFAAg3IXoWbk8V1MdYLrjgAnV0\ndKioqEgzZ84MdU0AgHDURc/C7SmCjv2FCxcqJSXlZ5/uAACAnQT1JKJTwSxcAOg5QjkL97+f/x/T\nx8YknmdhJV2Dr1QBAFgiXCcDmUWAAgCsEaaTgcwiQAEAlqADBQDADJt1oPa6WgAALEKAAgBgAkO4\nAABLOGz2IAUCFABgDSYRAQBw8hw2m0REgAIArGGzDjTkj/IDAOB0ZK9+GwAAixCgAACYQIACAGAC\nAQoAgAkEKAAAJhCgAACYcFoG6O7du5Wenq68vDzNmjVL2dnZeuKJJ7q7rJCw27XecccdkqSSkhI9\n99xzPtsnTpyoY8eOdUdpllu/fr3S09PV0dHhXbd48WJVVVX57PerX/2qq0sLmd27d8vpdKqlpcW7\nrqSkRNXV1d1YlbX27Nmj1atXd3cZsMhpGaCSNHbsWJWVlWnLli168cUXtWfPHu3Zs6e7ywoJO12r\nv+8bPJ2+i3D79u3q37+/3n33Xb/7nU7XLEnnnXeeSkpKursMy3366ac6cuSIdu3apbS0tE73effd\nd31+YULPd9oG6E+NHDlS9fX13V1Gl7DTtR7vdHkmyEcffSTDMDRnzhy9+uqr3V1Ol5o8ebIOHTp0\n2vz/e/jwYd1zzz164IEHFBERoT179mj06NFyu92aMWOGZs6cqUceeUSSdODAAblcLr344ovyeDzd\nXDmCEfaP8nv77be1adMmxcTEaMaMGTr77LP1j3/8w2eftrY2/f3vf9e0adO6qUprcK2+11pWVqbX\nX39d0g/hefzQXzjo7BoNw9Cbb76pqVOnasqUKSouLpbH41FUVJQkafXq1dq4caOk8P6Fwd/f78KF\nC7V69WqtXbu2m6s076OPPlJxcbEiIiJ0yy23aNSoUfJ4PPruu+8UExOjTZs26ZlnnlFiYqIqKirk\n8Xh0880368Ybb9Tzzz8vl8ulq6++WvPmzevuS4EfYR+ggwYN0rPPPqvDhw9r7dq1+uyzz5Sdna3d\nu3crLy9P33//verr63X33XdrxIgR3V3uKeFas332ycvL08yZM73LkyZN6uoyT0ln17hq1Spt375d\nzz77rM466yxdeOGFevvtt5WRkSFJuuuuuzRhwgTvOdLT07ur/FPyc3+/TU1N+uUvf6mNGzdq3759\n3V2maW+++aZ69+6thx9+WHFxcZKk/fv3a9SoUZKkadOm6dZbb9V1112nadOmeX9BiomJ0dy5czV4\n8GDde++9BGgPF/YBOmzYMEnSBRdc4J08s3v3bo0dO1Zr1qyRJLlcLu9+4Yxr3a0dO3Z0Z1mW6uwa\n9+7dq5aWFt1xxx0yDEPffPONtm/f7g3Q08XP/f02NTVJkgoKCrR06VKNHTu222o8Fbfccou2b9+u\n3/72t7rwwgt100036b333vNez80336zrrrtOf/7zn5Wfn6/nnntO/fr101//+ldt3rxZw4cP18sv\nv9zNV4FAbHEP9J577tFDDz3U3WV0CTtd60+F85Dmj1599VUtWrRIFRUVcrvdevXVV1VdXa3//ve/\nne5/OlxzZ4YNG6akpCRVVlZ2dymmRERE6Nprr1V5ebnGjx+ve++9V+vWrdOYMWNkGIaKi4s1YMAA\nzZ49W06nU59++qluu+02ffDBB3riiSdUVFSk5OTk7r4MBGCLAHU6nTr33HP10ksvdXcpIWena/2p\ncJ+R+v3336uystLn/vUZZ5yhyy+/XG+99Vanx4T7NfuzYMECHTlypLvLOGWXXnqpnnrqKY0cOVKx\nsbFyOByKjY1VTk6O5syZI4fDoZSUFK1du1YLFixQfHx8d5eMIPF1ZgAAmGCLDhQAAKsRoAAAmECA\nAgBgAgEKAIAJBCgAACYQoAAAmECAAgBgAgEKAIAJ/xfgKlqX+5E5MgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f50fd9678d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= R R IH AH N  \n",
      "> quit\n"
     ]
    }
   ],
   "source": [
    "interactive_session(model, vocab, i2w, show_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the attention weights show how important different parts of the input are for generating different tokens in the output. For tasks like machine translation, where the order of one-to-one words often changes due to grammatical differences between languages, this becomes very interesting as we see the attention window move further away from the diagonal that is mostly displayed in grapheme-to-phoneme translations.\n",
    "\n",
    "## What's next\n",
    "\n",
    "With the above model, you have the basics for training a powerful sequence-to-sequence model with attention in a number of distinct domains. The only major changes required are preparing a dataset with pairs input and output sequences and in general the rest of the building blocks will remain the same. Good luck, and have fun!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
